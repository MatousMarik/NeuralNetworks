{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36303b37-416b-4fa9-9407-56438d08ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()     # common random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b9a22-9522-4f63-8c80-fa605d8ab62a",
   "metadata": {},
   "source": [
    "# Assignment 2: $k$-Fold Cross-Validation\n",
    "\n",
    "Implement a function that compares two learning algorithms using $k$-fold \n",
    "cross-validation. A target function is \n",
    "$f: \\mathrm{R}^n \\to \\{0,1\\}$, where $\\mathrm{R}$ is the set of real numbers. The goal of a learning\n",
    "algorithm is to identify a function $h: \\mathrm{R}^n \\to \\{0,1\\}$, called\n",
    "hypothesis, from some class of functions $\\mathcal{H}$ such that the\n",
    "function $h$ is a good approximation of the target function $f$. The\n",
    "only information the algorithm can use is a sample $S\\subset X$ called\n",
    "a training set, and the correct value $f(x)$, called label, for all $x \\in S$.\n",
    "The sample $S$ is a set of $n$ elements from $X$ randomly selected\n",
    "according to some probabilistic distribution $D$.\n",
    "\n",
    "Suppose that each learning algorithm is implemented as two functions `learn()` and `apply()`:\n",
    "```\n",
    "def learn(X, y, par):\n",
    "    ...\n",
    "    return learned_par\n",
    "```\n",
    "where\n",
    "\n",
    "Param  |Meaning\n",
    "------ |---------------------------------------------------------------\n",
    "`learn`|is the name of the learning function,\n",
    "`X`    |is a two-dimensional numpy array (X_traing vectors are the rows of `X`),\n",
    "`y`    |is a vector of desired outputs (0/1), and\n",
    "`par`  |is a variable containing parameters of the learning algorithm.\n",
    "\n",
    "Such function returns learned parameters in the variable `learned_par`.\n",
    "\n",
    "Further, we will need a function\n",
    "\n",
    "```\n",
    "def apply(learned_par, X):\n",
    "    ...\n",
    "    return out\n",
    "```\n",
    "where\n",
    "\n",
    "Param  |Meaning\n",
    "------ |---------------------------------------------------------------\n",
    "`learned_par`|is a variable with the parameters learned by the function `learn`,\n",
    "`X`    |is a two-dimensional numpy array (X_traing vectors are the rows of `X`),\n",
    "\n",
    "\n",
    "that computes the learned function with parameters `learned_par` on input\n",
    "vectors from a two-dimensional numpy array `X` (each row of the array is an input\n",
    "vector). The returned value `out` is a vector of the results - zeros and ones.\n",
    "\n",
    "Then it is possible to implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f46a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def err(name_learn: str, name_apply: str, par: object, \n",
    "        X_train: np.ndarray, y_train: np.ndarray, \n",
    "        X_test: np.ndarray, y_test: np.ndarray) -> float:\n",
    "    '''Trains the algorithm name_learn with parameters par\n",
    "    on the train set X_train with the true labels y_train. Then,\n",
    "    it compares predictions computed by the trained function on the test set\n",
    "    X_test with the true labels y_test.\n",
    "    It returns the error rate on the test set.\n",
    "    \n",
    "        error_rate = err(name_learn, name_apply, par, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "    Args:\n",
    "        name_learn: The name of the training algorithm.\n",
    "        name_apply: The name of the function for applying the learned function.\n",
    "        par: The parameters of the learning algorithm.\n",
    "        X_train (2-d numpy array of floats): The training set; samples are rows.\n",
    "        y_train (vector of integers 0/1): The true labels for the training samples.\n",
    "        X_test (2-d numpy array of floats): The test set; samples are rows.\n",
    "        y_test (vector of integers 0/1): The true labels for the test samples.\n",
    "        \n",
    "    Returns:\n",
    "        error_rate: The error rate (a float between 0.0 and 1.0) of the function \n",
    "            with the name trained_apply on the test set X_test with correct \n",
    "            labels y_test after training using the function with the name\n",
    "            name_learned with parameters par on the train set X_train with the correct\n",
    "            labels y_train.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287450d2",
   "metadata": {},
   "source": [
    "that computes the error of the function `name_learned` learned by the\n",
    "algorithm `name_apply` with parameters `par` on a training set `X_train` and\n",
    "desired outputs `y_train`. The error is computed on a test set `X_test` with the\n",
    "desired outputs `y_test`. Note that the result of the function `err` is\n",
    "always a real value from the closed interval $\\langle0;1\\rangle$, **not\n",
    "the number of errors** made by the learned algorithm on the test set.\n",
    "`name_learn` and `name_learned` are strings containing the names of the respective\n",
    "functions. The respective functions can be evaluated using Python's function\n",
    "`eval`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa514787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "fun = \"math.log\"\n",
    "a = 16\n",
    "b = 2\n",
    "eval(fun+\"(a, b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e4ba9",
   "metadata": {},
   "source": [
    "Then it is easy to implement the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3ecc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(name_learn1: str, name_apply1: str, par1: object, \n",
    "              name_learn2: str, name_apply2: str, par2: object, \n",
    "              X: np.ndarray, y: np.ndarray, k: int, shuffle: bool = True) -> (float, float):\n",
    "    '''Computes the difference between errors and the standard deviation of the difference for\n",
    "    two learning algorithms.\n",
    "    \n",
    "        delta, std = cross_val(name_learn1, name_apply1, par1, \n",
    "                               name_learn2, name_apply2, par2, \n",
    "                               X, y, k, shuffle)\n",
    "                               \n",
    "    Args:\n",
    "        name_learn1: The name of the first training algorithm.\n",
    "        name_apply1: The name of the function for applying the first learned function.\n",
    "        par1: The parameters of the first learning algorithm.\n",
    "        name_learn2: The name of the second training algorithm.\n",
    "        name_apply2: The name of the function for applying the second learned function.\n",
    "        par2: The parameters of the second learning algorithm.\n",
    "        X (2-d numpy array of floats): The training set; samples are rows.\n",
    "        y (vector of integers 0/1): The desired outputs for the training samples.\n",
    "        k: The number of folds used in k-fold cross-validation.\n",
    "        shuffle: If True, shuffle the samples into folds; otherwise, do not shuffle\n",
    "                 the samples before splitting them into folds.\n",
    "    \n",
    "    Returns:\n",
    "        delta: The estimated difference: error rate of the first algorithm minus \n",
    "            the error rate of the second algorithm computed using k-fold cross-validation.\n",
    "        std: The standard deviation of the estimatied difference of errors.            \n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1508e3",
   "metadata": {},
   "source": [
    "that estimates the difference between the errors of the hypothesis\n",
    "`name_learned1` learned by a learning algorithm `name_learn1` with parameters `par1`\n",
    "and the error of the hypotheses `name_learned2` learned by a learning algorithm\n",
    "`name_learn2` with parameters `par2` using `k`-fold cross-validation on\n",
    "patterns `X` with the desired outputs `y`. The function returns the estimated\n",
    "difference of errors `delta` and estimated standard deviation `s` of this\n",
    "estimator. **You should implement your own function using `numpy`, not use \n",
    "any implementation of cross-validation from any third-party library!**\n",
    "\n",
    "If the value of `shuffle` is `False`, then\n",
    "the order of samples must not be changed before partitioning into folds\n",
    "for `k`-fold cross-validation, and all folds should be continuous parts\n",
    "of `X`. If the value of `shuffle` is `True`, then the patterns from `X` \n",
    "should be assigned randomly into `k` folds.\n",
    "\n",
    "*Note: The sizes of the folds can differ by at most 1.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ecdd5",
   "metadata": {},
   "source": [
    "For example, we can compare the errors of the perceptron learning\n",
    "algorithm limited to at most 10 epochs and the perceptron learning\n",
    "algorithm limited to at most 100 epochs. As the perceptron learning\n",
    "algorithm, we will use the function `perc_learn` obtained using our implementation \n",
    "of class `Perceptron` from the lab. Similarly, for applying the learned perceptron\n",
    "we will use `perc_apply` given below. For that, prepare a Python module `Perceptron` with the implementation of the class `Perceptron` from the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eabce994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Perceptron import Perceptron\n",
    "\n",
    "def perc_learn(X: np.ndarray, y: np.ndarray, par: list) -> Perceptron:\n",
    "    '''Train perceptron\n",
    "    \n",
    "        perc = perc_learn(X, y, par)\n",
    "        \n",
    "    Args:\n",
    "        X (2-d numpy array of floats): The training set; samples are rows.\n",
    "        y (vector of integers 0/1): The desired outputs for the training samples.\n",
    "        par: List of the parameters for the perceptron learning algorithm\n",
    "             [w, lr, max_epochs]:\n",
    "                 w (nd.array): The extended weight vector of a perceptron.\n",
    "                 lr (float): The learning rate of the perceptron.\n",
    "                 max_epochs (int): The maximal number of learning epochs.\n",
    "    \n",
    "    Returns:\n",
    "        perc (Perceptron): Trianed perceptron\n",
    "    '''\n",
    "    perc = Perceptron(init_weights=par[0])\n",
    "    perc.fit(X,y, lr=par[1], max_epochs=par[2])\n",
    "    return perc\n",
    "\n",
    "def perc_apply(learned_par: Perceptron, X: np.ndarray) -> np.ndarray:\n",
    "    '''Apply learned perceptron on inputs X.\n",
    "    \n",
    "        predictions = perc_apply(learned_par, X)\n",
    "        \n",
    "    Args:\n",
    "        learned_par (Perceptron): An instance of the class Perceptron.\n",
    "        X (2-d numpy array of floats): The training set; samples are rows.\n",
    "        \n",
    "    Returns:\n",
    "        predictions (vector of 0/1): predictions of the perceptron learned_par \n",
    "                                     on the test set X.\n",
    "    '''\n",
    "    Out = learned_par.predict(X)\n",
    "    return Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491db102",
   "metadata": {},
   "source": [
    "Another learning algorithm we will implement is `memorizer`, which\n",
    "memorizes all training samples and their true labels. Learned `memorizer` answers correctly on the inputs from the\n",
    "training set and randomly otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fcf75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memorizer_learn(X: np.ndarray, y: np.ndarray, par: object = None) -> list:\n",
    "    '''The learning algorithm that only remembers all training\n",
    "    samples and the desired answers for them.\n",
    "\n",
    "        learned_par = memorizer_learn(X, y, par)\n",
    "    \n",
    "    Args:\n",
    "        X (2-d numpy array of floats): The training set; samples are rows.\n",
    "        y (vector of integers 0/1): The desired outputs for samples from X.\n",
    "        par: It is not used here, it is present here for compatility only\n",
    "    \n",
    "    Retrurns:\n",
    "        learned_par (list): The list containing the training samples and\n",
    "            the desired outputs for them.\n",
    "    '''\n",
    "    return [X, y]\n",
    "\n",
    "def memorizer_apply(learned_par: list, X: np.ndarray):\n",
    "    '''A function simulating Memorizer\n",
    "    \n",
    "         out = memorizer_apply(learned_par, X)\n",
    "    \n",
    "    Args:\n",
    "        learned_par (list): List with training samples and the respective desired outputs.\n",
    "        X (2-d numpy array of floats): The test set; samples are rows.\n",
    "    \n",
    "    Returns:\n",
    "        out (vector of floats): whenever the i-th input vector is contained \n",
    "            within the memorized input samples, out[i] equals the i-th \n",
    "            remembered desired output, otherwise it will be randomly 0 or 1\n",
    "    '''\n",
    "    known = learned_par[0]\n",
    "    known_out = learned_par[1]\n",
    "    # generate random outputs\n",
    "    out = rng.integers(0, 2, size=X.shape[0])    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(known.shape[0]):\n",
    "            if np.array_equal(X[i], known[j,:]):\n",
    "                out[i] = known_out[i]\n",
    "                break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e20aca-c896-4dea-a976-d13a98886dc0",
   "metadata": {},
   "source": [
    "When applying $k$-fold cross-validation, we will compute the confidence interval for the estimation of the difference of errors computed by the `cross_val()` function. Implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4deaabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_interval(d: float, s: float, conf_level: float, k: int) -> tuple:\n",
    "    '''Compute confidence interval for the estimated difference of errors d \n",
    "    with standard deviation s returned from cross_val.\n",
    "    \n",
    "        low, high = conf_inteval(d, s, conf_level, k)\n",
    "    \n",
    "    Args:\n",
    "        d: The difference of errors computed by k-fold cross-validation.\n",
    "        s: The standard deviation of the difference of errors computed \n",
    "            by k-fold cross-validation.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30090732",
   "metadata": {},
   "source": [
    "The above algorithms can be compared using the above function\n",
    "`cross_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bad409-1b8a-4e40-bb92-0a0d40cc4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first we read training patterns; \n",
    "# this should be a matrix of size 600 x 2\n",
    "X1 = np.genfromtxt('X1.csv', delimiter=',', dtype = float)\n",
    "print(X1)\n",
    "print(X1.shape)\n",
    "\n",
    "# then we read the desired outputs for the X_traing patterns \n",
    "# (0/1 vector of length 600)\n",
    "y1 = np.genfromtxt('y1.csv', delimiter=',', dtype = int)\n",
    "print(y1)\n",
    "\n",
    "# a row X1[i] is the i-th training vector with \n",
    "# the desired output y1[i].\n",
    "\n",
    "# we prepare two lists of the learning parameters\n",
    "# for the perceptron learning algorithm consisting of:\n",
    "#     an extended weight vector,\n",
    "#     a learning rate, and\n",
    "#     a maximal number of epochs\n",
    "par1 = [[1, 1, -1], 1, 10]\n",
    "par2 = [[1, 1, -1], 1, 200]\n",
    "\n",
    "# run 5-fold cross-validation\n",
    "k = 5\n",
    "d, s = cross_val('perc_learn', 'perc_apply', par1, 'perc_learn', 'perc_apply',\n",
    "                par2, X1, y1, k, shuffle=False)\n",
    "\n",
    "print(f\"Estimated error difference {d}, standard deviation for the estimate is {s}\")\n",
    "# print the interval to which the true error difference belongs \n",
    "# with probability at least 95%\n",
    "print(f\"Confidence interval {conf_interval(d, s, 95, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39028576-7008-451a-aa85-1750a637bcbe",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "\n",
    "1.  Implement the functions `err`, `cross_val`, `conf_interval`, and by running the above     script,\n",
    "    estimate the difference in error rates of the perceptron learning\n",
    "    algorithm with at most 10 epochs and of the same perceptron learning\n",
    "    algorithm with at most 200 epochs. From the resulting error\n",
    "    difference and the standard deviation of the estimate, compute the\n",
    "    interval which contains the true error difference with the\n",
    "    probability of 95%. Is the error difference statistically significant?\n",
    "\n",
    "2.  Then modify the above script to compare error rates of the algorithm\n",
    "    memorizer and perceptron with at most 50 epochs, learning rate 1.0,\n",
    "    and initial extended weight vector `[1, 1, 1, 1, -1]` using 6-fold\n",
    "    cross-validation on the following 600 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3b29a72-f3c2-4613-b596-c5deb882049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "X2 = np.genfromtxt('X2.csv', delimiter=',', dtype = float)\n",
    "\n",
    "# labels for the samples\n",
    "y2 = np.genfromtxt('y2.csv', delimiter=',', dtype = int)\n",
    "\n",
    "# in order to obtain the same answers from memorizer_apply, we reset random \n",
    "# numbers generator before calling cross_val\n",
    "rng = default_rng(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27dfae4",
   "metadata": {},
   "source": [
    "**You should submit** a Jupyter notebook with \n",
    "\n",
    "* commented **source code** of the functions `err`, `cross_val`, `conf_interval`, and\n",
    "  all the functions you have used for solving the above tasks. Do not forget to attach your implementation of the class `Perceptron`.\n",
    "\n",
    "* The notebook should contain **the results** of your experiments for both above tasks. You should analyze\n",
    "  the obtained results, **answer the question** on the significance in both tasks. \n",
    "  \n",
    "* Write **a recommendation** which\n",
    "  algorithm is better to use. Eventually, you can advise which\n",
    "  experiments would be suitable for a more thorough comparison of the\n",
    "  considered learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f8325-a92e-4d7c-985b-30320c915d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
